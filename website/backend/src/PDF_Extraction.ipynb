{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "120fe54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/devansharora/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/devansharora/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/devansharora/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/Users/devansharora/anaconda3/lib/python3.11/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.0.0) was trained with spaCy v3.0.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/devansharora/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/devansharora/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/devansharora/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/devansharora/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/devansharora/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/devansharora/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/devansharora/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/devansharora/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/devansharora/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/devansharora/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/devansharora/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/devansharora/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "from document_retrieval_BM25 import get_query_results\n",
    "from doc_spell_checker import perform_correction\n",
    "from tf_idf_retrieval import retrieve_top_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36780e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(os.path.dirname(os.getcwd()), 'Dataset')\n",
    "csv_path = os.path.join(os.path.dirname(os.getcwd()),'res','csv_etl_files')\n",
    "pdf_paths = os.listdir(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "605ff7f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/devansharora/Desktop/IR/IR-Project/res/csv_etl_files'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c251a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A2019-17.pdf', 'A2009-12.pdf', 'A2009-06.pdf', 'A2012-35.pdf', 'A2017-10_3.pdf', 'A2016-36_0.pdf', 'A2010-31.pdf', 'A2013-18_1.pdf', 'A2016-30_3.pdf', 'A2016-16_0.pdf', 'A2015-11.pdf', 'A-2009-25.pdf', 'A2017-12_5.pdf', 'A2019-10.pdf', 'A2016-32_0.pdf', 'A2017-14_1.pdf', 'Indian Institutes of Management Act 2017.pdf', 'A2010-23.pdf', 'A2017-16_2.pdf', 'A2017-13_1.pdf', 'A2010-16_0.pdf', 'A2014-40_0.pdf', 'A2018-02_0.pdf', 'A2013-14.pdf', 'A2011-05.pdf', 'A2011-11.pdf', 'A2018-24_0.pdf', 'A2014-37.pdf', 'A2016-11_1.pdf', 'A2018-04_0.pdf', 'A2009-07_1.pdf', 'A2010-42.pdf', 'A2016-49_1.pdf', 'A2011-15.pdf', 'A2018-25.pdf', 'A2017-15_2.pdf', 'A2014-30.pdf', 'A2014-18.pdf', 'A2016-17_1.pdf', 'A2017-26_1.pdf', 'A2013-20.pdf', 'A2016-4_1.pdf', 'A2014-1.pdf', 'A2013-23.pdf', 'A2014-17.pdf', 'A2017-02_2.pdf', 'A2018-17.pdf', 'A2018-03.pdf', 'A2013-26.pdf', 'A2017-20_0.pdf', 'A2010-1.pdf', 'A2014-7.pdf', 'A2013-25.pdf', 'A2014-10.pdf', 'A2013-30.pdf', 'A2014-6.pdf', 'A2017-22_1.pdf', 'A2016-2_0.pdf', 'A2016-18_0.pdf', 'A2015-22.pdf', 'A2011-20_1.pdf', 'A2009-27.pdf', 'The Protection of Children from Sexual Offences Act, 2012_0.pdf', 'A2019-21.pdf', 'A2010-39.pdf', 'A2017-27_1.pdf', 'A2010-38.pdf', 'A2019-20.pdf', 'A2009-35_0.pdf', 'A2017-23_0.pdf', 'A2009-09.pdf', 'The Insolvency and Bankruptcy Code, 2016..pdf', 'A2012-38.pdf', 'A2012-12.pdf', 'A2010-19_0.pdf', 'A2016-38_1.pdf', 'A2012-13.pdf']\n"
     ]
    }
   ],
   "source": [
    "print(pdf_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "841a9b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PdfReader(file)\n",
    "        data = [(page.extract_text(), page_number + 1)\n",
    "                for page_number, page in enumerate(reader.pages)]\n",
    "    return data\n",
    "\n",
    "def process_data_and_save_to_dataframe(pdf_path):\n",
    "    data = extract_text_from_pdf(pdf_path)\n",
    "    processed_data = []\n",
    "\n",
    "    for page_text, page_no in data:\n",
    "        lines = page_text.split('\\n')\n",
    "        title = ''\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                if line.isupper() and line.strip('.').isalpha() and not line.isdigit():  \n",
    "                    title = line\n",
    "                else:\n",
    "                    processed_data.append({'Title': title, 'Paragraph Text': line, 'Page No': page_no})\n",
    "\n",
    "    dataframe = pd.DataFrame(processed_data)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa632ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, max_length=500, overlap=50, min_final_chunk_length=150):\n",
    "    # If the text is shorter than the max length, return it as is\n",
    "    if len(text) <= max_length:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    start_index = 0\n",
    "    \n",
    "    while start_index < len(text):\n",
    "        # End index is either the max length of the chunk or the end of the text\n",
    "        end_index = min(start_index + max_length, len(text))\n",
    "        \n",
    "        # If we are at the end of the text, append the chunk and break\n",
    "        if end_index == len(text):\n",
    "            chunks.append(text[start_index:end_index])\n",
    "            break\n",
    "        \n",
    "        # Find the last space in the text to potentially split at\n",
    "        split_index = text.rfind(' ', start_index, end_index + overlap)\n",
    "        \n",
    "        # If no space found, or if the split index is before the start index,\n",
    "        # just split at the max length without an overlap\n",
    "        if split_index <= start_index:\n",
    "            split_index = end_index\n",
    "        \n",
    "        # Append the chunk up to the split index\n",
    "        chunks.append(text[start_index:split_index])\n",
    "        \n",
    "        # Update the start index to continue from the split index\n",
    "        start_index = split_index\n",
    "\n",
    "    # After splitting, if the last chunk is smaller than the minimum size, merge it with the previous chunk\n",
    "    if len(chunks) > 1 and len(chunks[-1]) < min_final_chunk_length:\n",
    "        chunks[-2] += chunks[-1]  # Merge the last chunk with the second-to-last chunk\n",
    "        chunks = chunks[:-1]  # Remove the last chunk since it's now merged\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fea3537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combining_text_info(dataframe):\n",
    "    dataframe_dict = dataframe.to_dict(orient='list')\n",
    "    dataframe_dict_ret = {}\n",
    "    paragraph_start_pattern = re.compile(r'^(\\d+\\.\\s+.+)')\n",
    "    for i in dataframe_dict.keys():\n",
    "        dataframe_dict_ret[i] = []\n",
    "    current_combined_text = \"\"\n",
    "    current_paragraph_index = None\n",
    "    for index, row in dataframe.iterrows():\n",
    "        text = str(row['Paragraph Text']).strip()\n",
    "        if paragraph_start_pattern.match(text):\n",
    "            if current_paragraph_index is  None:\n",
    "                current_paragraph_index = index\n",
    "            else:\n",
    "                split_combined_text = split_text(current_combined_text)\n",
    "                title_lst = [dataframe_dict['Title'][current_paragraph_index]]*len(split_combined_text)\n",
    "                page_info_lst = [dataframe_dict['Page No'][current_paragraph_start_index]]*len(split_combined_text)\n",
    "                dataframe_dict_ret['Title'].extend(title_lst)\n",
    "                dataframe_dict_ret['Paragraph Text'].extend(split_combined_text)\n",
    "                dataframe_dict_ret['Page No'].extend(page_info_lst)\n",
    "            current_combined_text = \"\"\n",
    "            current_paragraph_start_index = index\n",
    "            \n",
    "        current_combined_text += text\n",
    "    \n",
    "    if len(current_combined_text) !=0:\n",
    "        dataframe_dict_ret['Title'].append(dataframe_dict['Title'][current_paragraph_index])\n",
    "        dataframe_dict_ret['Paragraph Text'].append(current_combined_text)\n",
    "        dataframe_dict_ret['Page No'].append(dataframe_dict['Page No'][index])\n",
    "    combined_dataframe = pd.DataFrame.from_dict(dataframe_dict_ret)\n",
    "    return combined_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18a581d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def pipeline(dataset_path,pdf_paths,csv_path):\n",
    "    for i in tqdm(range(len(pdf_paths))):\n",
    "        main_path = dataset_path+'/'+pdf_paths[i]\n",
    "        df = process_data_and_save_to_dataframe(main_path)\n",
    "        com_df = combining_text_info(df)\n",
    "        file_path = csv_path + '/'+pdf_paths[i][:-3]+'csv'\n",
    "        com_df.to_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4284d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 77/77 [00:36<00:00,  2.12it/s]\n"
     ]
    }
   ],
   "source": [
    "pipeline(dataset_path,pdf_paths,csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caf2afa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from doc_spell_checker import perform_correction\n",
    "from tqdm import tqdm\n",
    "def spell_correction_pipeline(csv_path):\n",
    "    csv_files = os.listdir(csv_path)\n",
    "    for i in tqdm(range(len(csv_files))):\n",
    "        csv_full_path = csv_path+'/'+csv_files[i]\n",
    "        if csv_full_path[-3:] != \"csv\":\n",
    "            continue\n",
    "        df = pd.read_csv(csv_full_path)\n",
    "        rows = len(df)\n",
    "        dict_df = df.to_dict(orient='list')\n",
    "        for j in range(rows):\n",
    "            dict_df['Paragraph Text'][j] = perform_correction(dict_df['Paragraph Text'][j])\n",
    "        df = pd.DataFrame.from_dict(dict_df)\n",
    "        df.to_csv(csv_full_path,index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6cadbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 78/78 [00:46<00:00,  1.67it/s]\n"
     ]
    }
   ],
   "source": [
    "spell_correction_pipeline(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "013dff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "def extract_filename(file_path):\n",
    "    # Use the os.path.basename function to get the filename from the file path\n",
    "    filename = os.path.basename(file_path)\n",
    "    return filename\n",
    "\n",
    "def tokenize(text):\n",
    "    # Basic tokenizer, could be enhanced with NLTK or similar libraries\n",
    "    return text.lower().split()\n",
    "\n",
    "def create_inverted_index(csv_files, inverted_index_path):\n",
    "    inverted_index = defaultdict(set)\n",
    "\n",
    "    for i in tqdm(range(len(csv_files))):\n",
    "        if csv_files[i][-3:] != 'csv':\n",
    "            continue\n",
    "        df = pd.read_csv(csv_files[i])\n",
    "        for j in range(len(df)):\n",
    "            paragraph = str(df['Paragraph Text'].iloc[j])\n",
    "            if paragraph:\n",
    "                for word in tokenize(paragraph):\n",
    "                    # Use the filename and row index as a unique identifier for the paragraph\n",
    "                    doc_id = extract_filename(csv_files[i])\n",
    "                    inverted_index[word].add(csv_files[i])\n",
    "    \n",
    "    with open(inverted_index_path, 'wb') as file:\n",
    "        pickle.dump(inverted_index, file)\n",
    "    return inverted_index\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'csv_files' is a list of paths to CSV files and 'inverted_index_path' is the path where the index should be saved.\n",
    "# inverted_index = create_inverted_index(csv_files, inverted_index_path)\n",
    "# Note: Actual file reading/writing would not work in this environment as file access is restricted.\n",
    "# This code is provided for illustrative purposes and should be run in a local Python environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "589896d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/devansharora/Desktop/IR/IR-Project/res/csv_etl_files/A2015-22.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 78/78 [00:00<00:00, 181.02it/s]\n"
     ]
    }
   ],
   "source": [
    "csv_files = os.listdir(csv_path)\n",
    "csv_files = [csv_path+\"/\"+csv for csv in csv_files]\n",
    "print(csv_files[0])\n",
    "inverted_index_path = os.path.join(os.path.dirname(os.getcwd()),'res')\n",
    "inverted_index_path = inverted_index_path+'/inverted_index.pkl'\n",
    "inverted_index = create_inverted_index(csv_files,inverted_index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "31a40214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8111"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ead01004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 78/78 [00:00<00:00, 843.83it/s]\n",
      "100%|████████████████████████████████████| 15783/15783 [00:45<00:00, 349.41it/s]\n",
      "100%|████████████████████████████████| 15783/15783 [00:00<00:00, 6012597.64it/s]\n"
     ]
    }
   ],
   "source": [
    "inverted_index_path = os.path.join(os.path.dirname(os.getcwd()),'res')\n",
    "inverted_index_path = inverted_index_path+'/inverted_index.pkl'\n",
    "inverted_index = create_inverted_index(inverted_index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6cfe2959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['s', 'h', 'o', 'r', 't', ' ', 'i', 'l', 'e', 'c', 'm', 'n', 'd', 'f', 'a', 'b', 'p', 'v', 'j', 'u', 'y', 'w', 'g', 'x', 'z', 'k', 'q'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24042cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = os.listdir(csv_path)\n",
    "file_paths = []\n",
    "for i in range(len(csv_files)):\n",
    "    if csv_files[i][-3:] == 'csv':\n",
    "        file_paths.append(csv_path+'/'+csv_files[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df5f7e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/devansharora/Desktop/IR/IR-Project/res/csv_etl_files/A2017-13_1.csv', '/Users/devansharora/Desktop/IR/IR-Project/res/csv_etl_files/A2011-15.csv', '/Users/devansharora/Desktop/IR/IR-Project/res/csv_etl_files/A2017-27_1.csv', '/Users/devansharora/Desktop/IR/IR-Project/res/csv_etl_files/A2017-15_2.csv', '/Users/devansharora/Desktop/IR/IR-Project/res/csv_etl_files/A2017-26_1.csv', '/Users/devansharora/Desktop/IR/IR-Project/res/csv_etl_files/A2016-11_1.csv', '/Users/devansharora/Desktop/IR/IR-Project/res/csv_etl_files/A2017-12_5.csv', '/Users/devansharora/Desktop/IR/IR-Project/res/csv_etl_files/A2017-14_1.csv', '/Users/devansharora/Desktop/IR/IR-Project/res/csv_etl_files/A2017-10_3.csv', '/Users/devansharora/Desktop/IR/IR-Project/res/csv_etl_files/A2014-17.csv']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "query=\"What is service charge?\"\n",
    "csv_path = os.path.join(os.path.dirname(os.getcwd()),'res','csv_etl_files')\n",
    "csv_files = os.listdir(csv_path)\n",
    "csv_files = [csv_path+'/'+csv for csv in csv_files]\n",
    "inverted_index_path = os.path.join(os.path.dirname(os.getcwd()),'res','inverted_index.pkl')\n",
    "with open(inverted_index_path, 'rb') as file:\n",
    "    inverted_index = pickle.load(file)\n",
    "top_files = retrieve_top_files(query,inverted_index,10)\n",
    "print(top_files)\n",
    "results = get_query_results(query,top_files,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d485c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30b18996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'service charge', 'retrieval_results': ['tax council constitute under the provision of article of the fund mean the good and service tax compensation fund refer to in section input tax in relation to a taxable person mean si ce charge on any supply of good or service or both make to ce charge on import of good and include the ce payable on reverse charge ba sish integrate good and service tax act mean the integrate good and service tax act of integrate tax mean the integrate good', 'good and service st ax act and the good and service tax compensation to state act if char ged separately by any amount that the supplier be liable to pay in relation to such supply but which have been incur red by the recipient of the supply and not include in the price actually pa id or payable for the goods or service or both c incidental expense include commission and pack charge by the supplier to the recipient of a supply and any amount charge for anything do by the supplier in respect of the supply of', 'and charge levy under any law for the time be in force other than this act and the goods and service tax compensation to state act if charge separately by the supplier provided also that in case where the penalty be leviable under the central good and service tax act and the state good and service tax act or the union territory good and service tax act the penalty leviable under this act shall be the sum total of the say also that where the appeal be to be file before the appellate authority or the appellate tribunal the maximum amount payable shall be fifty crore rupee and one hundred crore', 'right to legal aid a person with mental illness shall be entitle to receive free to exercise any of his right give under this it shall be the duty of magistrate police officer person in charge of such custodial institution as may be or medical officer or mental health professional in charge of a mental to inform the person with mental illness t hat he be entitle to free legal service under the legal service authority act of or other relevant law or under any order of the court if so order and provide the contact detail of the availability of service', 'tax to be first charge on property']}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5368177b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
