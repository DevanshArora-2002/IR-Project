import pandas as pd
import json
import os
from pprint import pprint
import bitsandbytes as bnb
import torch
import torch.nn as nn
import transformers
from huggingface_hub import notebook_login
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from document_retrieval_BM25 import get_query_results
from Embedding_Retrieval import get_top_retrieval
model_path = "mistralai/Mistral-7B-v0.1"
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",
    trust_remote_code=True,
    quantization_config=bnb_config
)
tokenizer = AutoTokenizer.from_pretrained(model_path)

def prompt2_generation(query,ranked_text):
  context = ""
  for i in range(len(ranked_text)):
      context += f"{i+1}: {ranked_text[i]}"

  prompt2 = f"""Given the a legal advice for this query: {query} 
              using the following contexual information {context}"""
  return prompt2
def generate_text2(prompt, tokenizer, model):
    """
    Generate text from a pre-trained language model given a prompt and a model name.

    Parameters:
    prompt (str): The prompt text to feed to the language model.
    model_name (str): The model identifier on Hugging Face's model hub.

    Returns:
    str: The text generated by the model.
    """

    # Encode the prompt text
    lst = [prompt]
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    input = tokenizer(lst, return_tensors='pt').to(device)
    print("Done")
    # Generate text using the model
    output = model.generate(
        **input,
        max_length=250,
        max_new_tokens=250,
        temperature=0.5,  # Adjust temperature for determinism
        top_p=0.95,  # Narrow down while allowing some diversity
        no_repeat_ngram_size=2  # Prevent repeating n-grams
    )

    # Decode the generated text
    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
    # cleaned_text = generated_text.split("Generate legal advice for")[1].strip()  # Simplified example
    return generated_text

def generate_fn(query):
    results = get_query_results(query,2)
    # results2 = get_top_retrieval(query,2)
    # results.extend(results2)
    prompt2 = prompt2_generation(query, results)
    final_text = generate_text2(prompt2, tokenizer, model)
    return final_text